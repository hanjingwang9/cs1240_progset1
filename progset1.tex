\documentclass[11pt]{article}
\setlength{\headheight}{13.599999pt}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}

% Page formatting
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\lhead{CS 1240}
\rhead{Kathy Jia \& Hanjing Wang}
\cfoot{\thepage}

\usepackage{xcolor}

% Title information
\title{CS 1240 Programming Assignment 1}
\author{Kathy Jia \& Hanjing Wang}
\date{February 18, 2026}

\begin{document}

\maketitle

\section{Quantitative Results}
\subsection{Table of Results}
\begin{table}[h!]
    \centering
    \begin{tabular}{|r|c|c|c|c|c|}
        \hline
        \textbf{n} & \textbf{Dim 0} & \textbf{Dim 1} & \textbf{Dim 2} & \textbf{Dim 3} & \textbf{Dim 4} \\
        \hline
        128    & 1.1986 & 20.0752    & 7.8473   & 18.0895  & 28.8345 \\
        256    & 1.1209 & 34.1532    & 10.7753  & 27.6187  & 47.1073 \\
        512    & 1.2365 & 66.1920    & 15.1069  & 43.1719  & 79.2117 \\
        1024   & 1.1942 & 116.4278   & 20.9311  & 68.0096  & 130.1026 \\
        2048   & 1.1986 & 215.5197   & 29.5082  & 107.4619 & 216.0709 \\
        4096   & 1.2114 & 387.7399   & 41.8154  & 168.9980 & 360.4264 \\
        8192   & 1.2157 & 724.0368   & 58.8329  & 267.2348 & 602.0764 \\
        16384  & 1.1945 & 1338.8172  & 83.2863  & 422.2005 & 1008.0576 \\
        32768  & 1.2079 & 2518.5486  & 117.4832 & 669.1275 & 1687.5327 \\
        65536  & -      & 4734.8078  & -        & -        & -        \\
        131072 & -      & 8935.7861  & -        & -        & -        \\
        262144 & -      & 16868.9239 & -        & -        & -        \\
        \hline
    \end{tabular}
    \caption{Average MST Weight by Dimension and Number of Points ($n$)}
    \label{tab:mst_results}
\end{table}

\begin{figure}[ht!]
    \centering
    % Row 1: Dim 0 and Dim 1
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mst_dim_0.png}
        \caption{Dimension 0 (Random Weights)}
        \label{fig:dim0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mst_dim_1.png}
        \caption{Dimension 1 (Hypercube)}
        \label{fig:dim1}
    \end{subfigure}
    
    \vspace{0.5cm} % Spacing between rows
    
    % Row 2: Dim 2 and Dim 3
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mst_dim_2.png}
        \caption{Dimension 2 (Euclidean)}
        \label{fig:dim2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mst_dim_3.png}
        \caption{Dimension 3 (Euclidean)}
        \label{fig:dim3}
    \end{subfigure}
    
    \vspace{0.5cm}
    
    % Row 3: Dim 4
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mst_dim_4.png}
        \caption{Dimension 4 (Euclidean)}
        \label{fig:dim4}
    \end{subfigure}
    
    \caption{MST Average Weight Scaling by Dimension. The blue lines represent experimental data, and the orange dashed lines represent the theoretical growth functions $f(n)$.}
    \label{fig:mst_scaling}
\end{figure}
\subsection{Asymptotic functions}

We give. 
\begin{itemize}
\item Dimension 0: \boldmath $f(n) = 1.2$ \unboldmath

\textit{Justification}: we know that the graphs in dimension 0 are complete, so there are $\frac{n(n-1)}{2} = O(n^2)$ edges and each of them are chosen uniformly from the interval $[0, 1]$. We note that there exists a constant $c \cdot n$ such that if we choose a set of $c \cdot n$ edges, we're almost certain to be able to build an MST for the graph. Then, supposing that we choose the smallest $c \cdot n$ edges and using order statistics, the weight of the largest edge in this set would be
$$\frac{cn}{n(n-1)/2 + 1} = \frac{2cn}{n(n-1) + 2} = \frac{2cn}{n(n-1) + 2}$$
Thus, the weight of the MST is
$$(n - 1) \cdot \frac{2cn}{n(n-1) + 2} = \frac{2cn\cdot(n - 1)}{n(n-1) + 2}$$
We note that as $n$ grows large, this is a fixed proportion $2c$. Using our experiments, we estimate this $2c$ to be $1.2$.

\item Dimension 1: \boldmath $f(n) = \frac{1.18 n}{\log n}$ \unboldmath

\textit{Justification}:  Unlike the previous dimension, the hypercube, each vertex only has $\log_2 n$ neighbors. Using order statistics, this then means that the cheapest edge for each vertex is around $\frac{1}{\log n + 1}$. Summing this over $n - 1$ edges gives a total weight of the MST proportional to $\frac{n -1}{\log n + 1} \approx c \cdot \frac{n}{\log n}$. Using our experiments, we estimate that $c = 1.18$.

\item Dimension 2: \boldmath $f(n) = 0.5 \sqrt{n}$ \unboldmath

\textit{Justification}: We know that the $n$ vertices are chosen randomly inside a unit square, which has area 1. This means on average, they each inhibit space of area $1/n$; we can picture this as a box, without loss of generality. Then, the distance between it and its nearest neighbor is simply the side length of this box, or $\sqrt{1/n}$. Therefore, the total weight of the MST is a sum of $n - 1$ edges, which is approximately

$$n \cdot \sqrt{1/n} = n \cdot n^{-1/2} = n^{1/2}.$$

\item Dimension 3: \boldmath $f(n) = 0.65 n^{2/3}$ \unboldmath

\textit{Justification}: We know that the $n$ vertices are chosen randomly inside a unit cube of volume 1. Similar to dimension 2, this means each vertex inhibit a cubic space of $1/n$ on average. Then, the distance between it and its nearest neighbor is simply the side length of this cube, or $\sqrt[3]{1/n}$. Therefore, the total weight of the MST is a sum of $n - 1$ edges, which is approximately

$$n \cdot \sqrt[3]{1/n} = n \cdot n^{-1/3} = n^{2/3}.$$

\item Dimension 4: \boldmath $f(n) = 0.69 n^{3/4}$ \unboldmath

\textit{Justification}: We know that the $n$ vertices are chosen randomly inside a unit hypercube of hypervolume 1. Similar to dimensions 2 and 3, this means each vertex inhibit a hypercubic space of $1/n$ on average. Then, the distance between it and its nearest neighbor is $\sqrt[4]{1/n}$. Therefore, the total weight of the MST is a sum of $n - 1$ edges, which is approximately

$$n \cdot \sqrt[4]{1/n} = n \cdot n^{-1/4} = n^{3/4}.$$


\end{itemize}

\section{MST Algorithm Runtime}
\subsection{Optimization using $k(n)$}
We simply divide our $f(n)$ functions (without the experimentally determined constant) by $n$ and, after simplifying, change the numerator from $1$ to $\log n$ to ensure that we're not removing too many edges from each individual vertex. For dimension 1, that ends up being $1$, so we don't end up imposing a limit.

Crucially, this will still lead to the correct MST when we run Kruskal's algorithm. This is because our algorithm sorts the edges by weight, so an edge with weight $w > k(n)$ would only be used if it were necessary to connect two components that couldn't be connected by cheaper edges. Our threshold almost guarantees that the included cheap edges already connect the entire graph, so more expensive edges would be discarded by Kruskal's anyway. Therefore, preemptively discarding them changes nothing about the final result, but drastically improves runtime of Kruskal's algorithm.

\subsection{Asymptotic Runtime}
\textbf{Graph Construction}

For dimension 0, we visit every single pair regardless of edge weight and decide whether or not to discard it by comparing it to $k(n)$. This means that the worst-case time is equal to the expected time of $O(n^2)$. The threshold $k(n) = \frac{2 \log n}{n}$ ensures that the expected number of edges is $\frac{2 \log n}{n} \cdot O(n^2)$, or $M = O(n \log n)$.

For dimension 1, we visit every vertex and for each, we iterate $\log n$ neighbors. This means that the worst-case time is equal to the expected time of $O(n\log n)$. Since this graph does not discard any edges, the total number of edges is $M = O(n\log n)$.

For dimensions 2, 3, and 4, we sort the points and, for each point, iterate through its neighbors until the $x$-coordinate distances is greater than $k(n)$. The worst case here is $O(n^2)$, where we iterate through all of each vertex's neighbors.



Our disjoint-set union-find data structure which implements both path compression and union by rank during \texttt{unite}. This ensures that the time complexity is $O(N)$ for each find.
We know that Kruskal's algorithm runs in time $O(|E|\log|V|)$

\section{Discussion}

\end{document}